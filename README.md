This was completed as part of Task 6 of an AI/ML internship. It focuses on using the K-Nearest Neighbors (KNN) algorithm with the Iris dataset. The dataset was loaded from a CSV file and processed by changing the categorical species labels into numeric values. Then, the feature columns were normalized with StandardScaler to ensure fair distance calculations. After splitting the data into training and test sets with a 70:30 ratio, KNN models were trained using different values of K (1, 3, 5, 7, and 9). The performance of each model was measured using accuracy scores and confusion matrices. The results showed that the model achieved 100% accuracy for several K values. Confusion matrices confirmed perfect classification across all classes, especially with K=3 and K=5. Furthermore, decision boundaries were plotted using the first two features to visualize how the algorithm separates classes. The setosa class had clear linear separation. This project provided a hands-on understanding of how KNN works, how the choice of K affects the model, and why normalization is important in distance-based learning methods.
